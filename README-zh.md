# NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models under Data Constraints (NeurIPS 2025)

[[ğŸ“œ Paper]](xxx) [[â­ï¸Project Page]](xxx) [[ğŸ¤— Model]](https://huggingface.co/collections/OpenGVLab/navil-68e62e7d20ea3e4097b56778) [[ğŸ“ English Version]](README.md)

## ğŸ“– æ‘˜è¦

åœ¨ç°æœ‰çš„å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ä¸­ï¼Œç»„åˆå¼è®­ç»ƒï¼ˆCompositional Trainingï¼‰å·²æˆä¸ºä¸»æµèŒƒå¼ï¼Œå®ƒé€šè¿‡æŒç»­çš„å¤šæ¨¡æ€é¢„è®­ç»ƒå°†é¢„è®­ç»ƒå¥½çš„è§†è§‰ç¼–ç å™¨ä¸è¯­è¨€æ¨¡å‹è¿æ¥èµ·æ¥ã€‚ç„¶è€Œï¼Œç”±äºå…¶åˆ†ç¦»å¼çš„è®­ç»ƒæ–¹å¼ï¼Œæ¢ç´¢è¿™ç§èŒƒå¼çš„å¤šæ¨¡æ€æ‰©å±•å±æ€§ï¼ˆScaling Propertyï¼‰å˜å¾—ååˆ†å›°éš¾ã€‚

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸“æ³¨äºä»¥ç«¯åˆ°ç«¯æ–¹å¼è¿›è¡Œ MLLM çš„åŸç”Ÿè®­ç»ƒï¼ˆNative Trainingï¼‰ï¼Œå¹¶ç³»ç»Ÿæ€§åœ°ç ”ç©¶äº†åœ¨æ•°æ®å—é™è¿™ä¸€å®é™…æƒ…å†µä¸‹çš„æ¨¡å‹è®¾è®¡ç©ºé—´å’Œæ‰©å±•å±æ€§ã€‚é€šè¿‡å¯¹ MLLM ä¸­å„ç§è®¾è®¡é€‰æ‹©çš„æ·±å…¥ç ”ç©¶ï¼Œæˆ‘ä»¬æ‰¾åˆ°äº†ä¸€ä¸ªèƒ½å¤Ÿæœ€ä½³å¹³è¡¡æ€§èƒ½ä¸è®­ç»ƒæˆæœ¬çš„å…ƒæ¶æ„ï¼ˆMeta-architectureï¼‰ã€‚åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç´¢äº†åŸç”Ÿ MLLM çš„æ‰©å±•è§„å¾‹ï¼Œå¹¶æ­ç¤ºäº†è§†è§‰ç¼–ç å™¨å’Œè¯­è¨€æ¨¡å‹ä¹‹é—´å­˜åœ¨æ­£ç›¸å…³çš„æ‰©å±•å…³ç³»ã€‚

åŸºäºè¿™äº›å‘ç°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåä¸º **NaViL** çš„åŸç”Ÿ MLLMï¼Œå¹¶ç»“åˆäº†ä¸€å¥—ç®€æ´ä¸”ç»æµé«˜æ•ˆçš„è®­ç»ƒæ–¹æ¡ˆã€‚åœ¨ 14 ä¸ªå¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸Šçš„å®éªŒç»“æœè¯å®ï¼ŒNaViL çš„æ€§èƒ½ä¸ç°æœ‰é¡¶å°–çš„ MLLM ç›¸å½“ã€‚æˆ‘ä»¬çš„å‘ç°å’Œæˆæœä¸ºæœªæ¥åŸç”Ÿ MLLM çš„ç ”ç©¶æä¾›äº†æ·±åˆ»çš„è§è§£ã€‚

## ğŸ’¡ æ ¸å¿ƒæ´è§ (Core Insights)

æˆ‘ä»¬å¯¹åŸç”Ÿ MLLM çš„è®¾è®¡å’Œæ‰©å±•å±æ€§è¿›è¡Œäº†ç³»ç»Ÿæ€§ç ”ç©¶ï¼Œå¾—å‡ºäº†äº”ä¸ªå…³é”®ç»“è®ºï¼Œè¿™äº›ç»“è®ºæŒ‡å¯¼äº† NaViL çš„è®¾è®¡ï¼š

1.  **LLM åˆå§‹åŒ–è‡³å…³é‡è¦**: ä»ä¸€ä¸ªé¢„è®­ç»ƒå¥½çš„ LLM åˆå§‹åŒ–æ¨¡å‹ï¼Œèƒ½å¤Ÿæå¤§åœ°åŠ é€Ÿå¤šæ¨¡æ€è®­ç»ƒçš„æ”¶æ•›ï¼Œå¹¶ä¸”å³ä¾¿åœ¨æ‹¥æœ‰å¤§é‡å¤šæ¨¡æ€æ•°æ®çš„æƒ…å†µä¸‹ï¼Œå…¶æ€§èƒ½ä¹Ÿé€šå¸¸ä¼˜äºä»é›¶å¼€å§‹è®­ç»ƒçš„æ¨¡å‹ã€‚

<p align="center">
<img src="images/comparison_llm_init.png" alt="LLM Initialization Comparison" style="width: 80%; height: auto;" />
</p>

2.  **MoE æ¶æ„è¡Œä¹‹æœ‰æ•ˆ**: æ··åˆä¸“å®¶æ¨¡å‹ï¼ˆMoEï¼‰èƒ½åœ¨ä¸å¢åŠ æ¨ç†æˆæœ¬ï¼ˆæ¿€æ´»å‚æ•°é‡ï¼‰çš„å‰æä¸‹ï¼Œæ˜¾è‘—æå‡æ¨¡å‹å¤„ç†å¼‚æ„æ•°æ®çš„èƒ½åŠ›å’Œæ•´ä½“æ€§èƒ½ã€‚æˆ‘ä»¬å‘ç°ï¼Œä¸ºæ³¨æ„åŠ›å’Œå‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰åŒæ—¶å¼•å…¥æ¨¡æ€ç‰¹å®šçš„ä¸“å®¶ï¼ˆModality-specific Expertsï¼‰æ•ˆæœæœ€ä½³ã€‚

<p align="center">
<img src="images/comparison_moe.png" alt="MoE Architecture Comparison" style="width: 60%; height: auto;" />
</p>

3.  **è§†è§‰ç¼–ç å™¨æ¶æ„çš„çµæ´»æ€§**: åœ¨ç»™å®šçš„å‚æ•°é¢„ç®—ä¸‹ï¼Œè§†è§‰ç¼–ç å™¨çš„æ€§èƒ½åœ¨ä¸€ç³»åˆ—å¹¿æ³›çš„æ·±åº¦å’Œå®½åº¦é…ç½®ä¸­éƒ½æ¥è¿‘æœ€ä¼˜ã€‚è¾ƒæµ…çš„ç¼–ç å™¨åœ¨è®­ç»ƒæ—©æœŸæ”¶æ•›æ›´å¿«ï¼Œè€Œè¾ƒæ·±çš„ç¼–ç å™¨åœ¨æ‹¥æœ‰æ›´å¤šæ•°æ®æ—¶è¡¨ç°ç•¥å¥½ã€‚

4.  **éå¯¹ç§°çš„æ‰©å±•æ•ˆåº”**: æ‰©å±• LLM çš„è§„æ¨¡èƒ½å¤ŸæŒç»­å¸¦æ¥å¤šæ¨¡æ€æ€§èƒ½çš„æå‡ï¼Œç¬¦åˆä¼ ç»Ÿçš„è¯­è¨€æ¨¡å‹æ‰©å±•å®šå¾‹ã€‚ç„¶è€Œï¼Œæ‰©å±•è§†è§‰ç¼–ç å™¨çš„æ”¶ç›Šæ˜¯é€’å‡çš„ï¼Œå…¶æ€§èƒ½ä¸Šé™å—é™äº LLM çš„å®¹é‡ã€‚

5.  **è§†è§‰ä¸è¯­è¨€çš„è”åˆæ‰©å±•å®šå¾‹**: æˆ‘ä»¬çš„ç ”ç©¶é¦–æ¬¡æ­ç¤ºï¼Œ**è§†è§‰ç¼–ç å™¨çš„æœ€ä¼˜è§„æ¨¡ä¸ LLM çš„è§„æ¨¡åœ¨å¯¹æ•°å°ºåº¦ä¸Šæˆæ­£æ¯”**ã€‚è¿™æ„å‘³ç€ä¸¤è€…åº”å½“è¢«è”åˆæ‰©å±•ï¼Œä¹ŸæŒ‡å‡ºäº†ç°æœ‰ç»„åˆå¼ MLLM ä¸ºä¸åŒå°ºå¯¸çš„ LLM é…å¤‡å›ºå®šå¤§å°è§†è§‰ç¼–ç å™¨çš„æ¬¡ä¼˜æ€§ã€‚

<p align="center">
<img src="images/comparison_vit_size_vs_llm_size.png" alt="Visual Encoder vs LLM Scaling" style="width: 60%; height: auto;" />
</p>

æ›´å¤šå†…å®¹è¯·å‚è§åŸæ–‡ [paper](xxx).

## ğŸ—ï¸ NaViL æ¶æ„

åŸºäºä¸Šè¿°æ´è§ï¼Œæˆ‘ä»¬æ„å»ºäº† NaViLã€‚å®ƒæ˜¯ä¸€ä¸ªåŸç”Ÿçš„ã€åŸºäº MoE çš„ MLLMï¼Œå¯ä»¥è¿›è¡Œç«¯åˆ°ç«¯çš„è®­ç»ƒï¼Œå¹¶åŸç”Ÿæ”¯æŒä»»æ„åˆ†è¾¨ç‡çš„å›¾åƒè¾“å…¥ã€‚

<p align="center">
<img src="images/arch.png" alt="NaViL Architecture Diagram" style="width: 100%; height: auto;" />
</p>

-   **è§†è§‰ç¼–ç å™¨ (Visual Encoder)**: è´Ÿè´£åˆæ­¥æå–è§†è§‰ä¿¡æ¯ã€‚
-   **MLP è¿æ¥å™¨ (MLP Connector)**: å°†è§†è§‰ç‰¹å¾æŠ•å½±åˆ° LLM çš„ç‰¹å¾ç©ºé—´ã€‚
-   **MoE æ‰©å±•çš„ LLM**: åŒ…å«æ¨¡æ€ç‰¹å®šçš„æ³¨æ„åŠ›ï¼ˆMHA-MMoEï¼‰å’Œå‰é¦ˆç½‘ç»œï¼ˆFFN-MMoEï¼‰ï¼Œä»¥æ›´ä¼˜çš„æ–¹å¼èåˆè§†è§‰å’Œæ–‡æœ¬ä¿¡æ¯ã€‚
-   **è§†è§‰å¤šå°ºåº¦æ‰“åŒ… (Visual Multi-scale Packing)**: åœ¨æ¨ç†é˜¶æ®µï¼Œé€šè¿‡å¤„ç†å¤šå°ºåº¦çš„å›¾åƒè¾“å…¥ï¼Œè¿›ä¸€æ­¥æå‡æ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“Š ä¸»è¦ç»“æœ

æˆ‘ä»¬åœ¨ 14 ä¸ªä¸»æµçš„å¤šæ¨¡æ€åŸºå‡†æµ‹è¯•ä¸Šå¯¹ NaViL è¿›è¡Œäº†å…¨é¢è¯„ä¼°ï¼Œæ¶µç›–äº†é€šç”¨èƒ½åŠ›ã€è§†è§‰é—®ç­”ã€OCRã€å›¾è¡¨å’Œæ–‡æ¡£ç†è§£ç­‰å¤šä¸ªç»´åº¦ã€‚

### ä¸ SOTA æ¨¡å‹çš„æ¯”è¾ƒ

NaViL-2B å’Œ NaViL-9B åœ¨ç›¸è¿‘çš„å‚æ•°è§„æ¨¡ä¸‹ï¼Œ**å¹³å‡æ€§èƒ½è¶…è¶Šäº†æ‰€æœ‰å·²æœ‰çš„åŸç”Ÿ MLLM**ï¼Œå¹¶è¾¾åˆ°äº†ä¸é¡¶å°–ç»„åˆå¼ MLLMï¼ˆå¦‚ InternVL-2.5, Qwen2.5-VLï¼‰ç›¸åª²ç¾çš„æ°´å¹³ï¼Œå±•ç¤ºäº†æˆ‘ä»¬æå‡ºçš„åŸç”Ÿè®­ç»ƒèŒƒå¼å’Œæ‰©å±•å®šå¾‹çš„ä¼˜è¶Šæ€§ã€‚

| æ¨¡å‹ | æ¿€æ´»å‚æ•°é‡ | å¹³å‡åˆ† | MMVet | MMMU | MMB | MME | MathVista | OCR-B | TextVQA | DocVQA | AI2D | ChartQA | InfoVQA |
| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |
| **_Compositional MLLMs_** |
| [Qwen2.5-VL](https://github.com/QwenLM/Qwen-VL) | 8.2B | 80.2 | 67.1 | 58.6 | 83.5 | 2347 | 68.2 | 864 | 84.9 | 95.7 | 83.9 | 87.3 | 82.6 |
| [InternVL-2.5](https://github.com/OpenGVLab/InternVL) | 8.1B | 77.3 | 62.8 | 56.0 | 84.6 | 2344 | 64.4 | 822 | 79.1 | 91.9 | 84.5 | 84.8 | 75.7 |
| **_Native MLLMs_** |
| [EVEv2](https://github.com/baaivision/EVE) | 7B | 62.3 | 45.0 | 39.3 | 66.3 | 1709 | 60.0* | 702 | 71.1 | 77.4* | 74.8 | 73.9 | 45.8* |
| [SAIL](https://github.com/ByteDance-Seed/SAIL) | 7B | 63.7 | 46.3 | 38.6* | 70.1 | 1719 | 57.0 | 783 | 77.1 | 78.4* | 76.7 | 69.7* | 47.3* |
| **NaViL-2B (ours)** | **2.4B** | **68.8** | **78.3** | **41.8** | **71.2** | **1822** | **50.0** | **796** | **76.9** | **85.4** | **74.6** | **78.0** | **56.0** |
| **NaViL-9B (ours)** | **9.2B** | **77.0** | **79.6** | **54.7** | **76.5** | **2225** | **66.7** | **837** | **77.2** | **90.6** | **82.4** | **85.4** | **70.2** |

> * \* ä¸ºæˆ‘ä»¬åœ¨æœ¬åœ°ä½¿ç”¨ [VLMEvalKit](https://github.com/open-compass/VLMEvalKit) ä»¥åŠ [OpenCompass](https://rank.opencompass.org.cn/leaderboard-multimodal/?m=REALTIME) è¿›è¡Œæµ‹è¯•çš„ç»“æœã€‚
> * å¹³å‡åˆ†é€šè¿‡å°†æ¯ä¸ªæŒ‡æ ‡å½’ä¸€åŒ–è‡³ 0-100 åŒºé—´æ¥è®¡ç®—ã€‚

### å®šæ€§åˆ†æ

æˆ‘ä»¬é€šè¿‡å¯è§†åŒ–æ³¨æ„åŠ›å›¾å‘ç°ï¼Œä¸€ä¸ªå°ºå¯¸è¶³å¤Ÿå¤§çš„è§†è§‰ç¼–ç å™¨ï¼ˆéµå¾ªæˆ‘ä»¬çš„è”åˆæ‰©å±•å®šå¾‹ï¼‰èƒ½å¸®åŠ©æ¨¡å‹åœ¨æ›´æµ…çš„å±‚å°±å¼€å§‹å…³æ³¨å…¨å±€ä¿¡æ¯ï¼Œå¹¶ä¿ƒè¿›è§†è§‰ä¸æ–‡æœ¬ç‰¹å¾æ›´æ—©åœ°è¿›è¡Œäº¤äº’ï¼Œè¿™ä¸ºæ¨¡å‹æ€§èƒ½çš„æå‡æä¾›äº†è§£é‡Šã€‚

<p align="center">
<img src="images/visualization_attention_matrix.png" alt="Attention Map Visualization" style="width: 100%; height: auto;" />
</p>
* ä¸Šï¼šä½¿ç”¨150Mè§†è§‰ç¼–ç å™¨ï¼›ä¸‹ï¼šä½¿ç”¨1.2Bè§†è§‰ç¼–ç å™¨ã€‚åè€…åœ¨æµ…å±‚ï¼ˆLayer 1ï¼‰å°±è¡¨ç°å‡ºæ›´å¼ºçš„å…¨å±€æ³¨æ„åŠ›å’Œè·¨æ¨¡æ€äº¤äº’ã€‚*

## ğŸš€ å¼€å§‹ä½¿ç”¨

```bash
# 1. å…‹éš†ä»“åº“
git clone https://github.com/OpenGVLab/NaViL.git
cd NaViL

# 2. åˆ›å»ºå¹¶æ¿€æ´» conda ç¯å¢ƒ
conda create -n navil python=3.10 -y
conda activate navil

# 3. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 4. è¿è¡Œæ¨ç† demo

## 2B ç‰ˆæœ¬
python -u demo.py --model_name_or_path OpenGVLab/NaViL-2B
## 9B ç‰ˆæœ¬
python -u demo.py --model_name_or_path OpenGVLab/NaViL-9B
```

## âœ¨ æ¨ç†ç¤ºä¾‹


ä»¥ä¸‹æ˜¯åŸºäº `transformers` åº“ä½¿ç”¨ NaViL è¿›è¡Œå¤šæ¨¡æ€é—®ç­”çš„ç¤ºä¾‹ä»£ç ã€‚

> è¯·ä½¿ç”¨ transformers==4.51.0 ç‰ˆæœ¬ä»¥ç¡®ä¿æ¨¡å‹èƒ½æ­£å¸¸å·¥ä½œã€‚

<details>
<summary>æ¨ç†ç¤ºä¾‹ä»£ç  (ç‚¹å‡»å±•å¼€)</summary>


```python
import torch
from transformers import AutoTokenizer, AutoModel
from PIL import Image

def anyres_preprocess_multi_scale(images, image_processor, max_pixels=-1, min_pixels=-1, scale_downsample_ratio=0.7071):
    assert min_pixels > 0 and max_pixels > 0, 'min_pixels and max_pixels must be set'
    if not isinstance(images, list):
        images = [images]
    
    pixel_values_all, image_grid_thws_all, num_scales_all = [], [], []
    for image in images:
        ret = image_processor(image, return_tensors="pt", min_pixels=min_pixels, max_pixels=max_pixels)
        image_grid_thws = [ret['image_grid_thw'][0]]
        pixel_values = ret['pixel_values'].reshape(ret['image_grid_thw'].prod(), -1, image_processor.patch_size, image_processor.patch_size)

        while True:
            current_pixels = image_grid_thws[0].prod() * (image_processor.patch_size ** 2)
            max_pixels = current_pixels * (scale_downsample_ratio ** 2)
            if max_pixels < min_pixels:
                break
            ret = image_processor(image, return_tensors="pt", min_pixels=min_pixels, max_pixels=max_pixels)
            if ret['image_grid_thw'].prod() >= image_grid_thws[0].prod():
                break
            image_grid_thws.insert(0, ret['image_grid_thw'][0])
            pixel_values = torch.cat([ret['pixel_values'].reshape(ret['image_grid_thw'].prod(), -1, image_processor.patch_size, image_processor.patch_size), pixel_values], dim=0)
            
        pixel_values_all.append(pixel_values)
        image_grid_thws_all.extend(image_grid_thws)
        num_scales_all.append(len(image_grid_thws))
    pixel_values = torch.cat(pixel_values_all, dim=0)
    return pixel_values, image_grid_thws_all, num_scales_all


def load_image(
    image_files,
    image_processor,
    patch_size=16,
    max_num=24576,
    min_num=256,
    upscale=False,
    scale_downsample_ratio=0.7071,
):
    
    if not isinstance(image_files, list):
        image_files = [image_files]
    
    images = []
    for image_file in image_files:
        image = Image.open(image_file).convert('RGB')
        if upscale:
            image = image.resize((image.width * 2, image.height * 2), Image.BILINEAR)
        images.append(image)
    
    min_pixels = min_num * (patch_size ** 2)
    max_pixels = max_num * (patch_size ** 2)
    pixel_values, image_grid_thws, num_scales = anyres_preprocess_multi_scale(
                                                                images=images,
                                                                image_processor=image_processor,
                                                                max_pixels=max_pixels,
                                                                min_pixels=min_pixels,
                                                                scale_downsample_ratio=scale_downsample_ratio,
                                                            )

    image_grid_thws = torch.stack(image_grid_thws)
    num_scales = torch.tensor(num_scales)
    return pixel_values, image_grid_thws, num_scales


def load_model_tokenizer(model_path):
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, use_fast=False)

    device = torch.cuda.current_device()
    model = AutoModel.from_pretrained(
        model_path,
        low_cpu_mem_usage=True,
        torch_dtype=torch.bfloat16,
        trust_remote_code=True,
        load_in_8bit=False
    ).eval()
    model.init_special_token_ids(tokenizer)

    # fix bug caused by size mismatch
    if hasattr(model.config, "tie_word_embeddings") and model.config.tie_word_embeddings:
        model.language_model.tie_weights()

    model = model.to(device)

    return model, tokenizer


def generate(message, model, tokenizer):
    image_num = len([x for x in message if x['type'] == 'image'])
    prompt = '\n'.join([x['value'] for x in message if x['type'] == 'text'])

    if image_num > 0:
        image_paths = [x['value'] for x in message if x['type'] == 'image']
        pixel_values, image_grid_thws, num_scales = load_image(
                                                                image_paths,
                                                                model.image_processor,
                                                                max_num=model.config.max_dynamic_patch,
                                                                min_num=model.config.min_dynamic_patch,
                                                                patch_size=model.config.vision_config.patch_size,
                                                                scale_downsample_ratio=model.config.scale_downsample_ratio,
                                                            )
        pixel_values = pixel_values.cuda().to(torch.bfloat16)
        image_grid_thws = image_grid_thws.cuda()
        num_scales = num_scales.cuda()
    else:
        pixel_values, image_grid_thws, num_scales = None, None, None

    generation_config = dict(do_sample=False, max_new_tokens=1024, top_p=None, num_beams=1)
    with torch.no_grad():
        try:
            response = model.chat(
                tokenizer,
                pixel_values=pixel_values,
                question=prompt,
                generation_config=generation_config,
                verbose=True,
                anyres_image_size=True,
                num_patches_list=image_grid_thws,
                num_scales=num_scales,
                )
        except Exception as e:
            print(f"Error in model chat: {e}")
            raise e
    return response

# --- ä¸»ç¨‹åº ---
# é€‰æ‹©è¦åŠ è½½çš„æ¨¡å‹
# model_path = "OpenGVLab/NaViL-2B"
model_path = "OpenGVLab/NaViL-9B"

print(f"Loading model from {model_path}...")
model, tokenizer = load_model_tokenizer(model_path)

# å‡†å¤‡è¾“å…¥æ¶ˆæ¯
# è¾“å…¥æ ¼å¼ä¸ºå­—å…¸åˆ—è¡¨ï¼Œæ”¯æŒå¤šå¼ å›¾ç‰‡å’Œå¤šæ®µæ–‡æœ¬
message = [
    {"type": "image", "value": "./examples/image1.jpg"},
    {"type": "text", "value": "Please describe the image shortly."},
]

print("Generating response...")
response = generate(message, model, tokenizer)

print("\n=== Response ===")
print(response)

```

</details>

## âœï¸ å¦‚ä½•å¼•ç”¨

å¦‚æœæ‚¨åœ¨æ‚¨çš„ç ”ç©¶ä¸­ä½¿ç”¨äº† NaViL æˆ–æˆ‘ä»¬çš„å‘ç°ï¼Œè¯·è€ƒè™‘å¼•ç”¨æˆ‘ä»¬çš„è®ºæ–‡ï¼š

```bibtex
@article{tian2025navil,
  title={NaViL: Rethinking Scaling Properties of Native Multimodal Large Language Models under Data Constraints},
  author={Tian, Changyao and Li, Hao and Luo, Gen and Zhu, Xizhou and Su, Weijie and Deng, Hanming and Zhu, Jinguo and Shao, Jie and Zhu, Ziran and Liu, Yunpeng and Lu, Lewei and Wang, Wenhai and Li, Hongsheng and Dai, Jifeng},
  journal={arXiv preprint},
  year={2025}
}
```
